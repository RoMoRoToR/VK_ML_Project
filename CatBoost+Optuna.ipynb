{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import catboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import pymorphy2\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, classification_report, accuracy_score\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import warnings\n",
    "from sklearn.cluster import DBSCAN\n",
    "import optuna\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Загрузка данных\n",
    "train_groups = pd.read_csv('/mnt/data/train_groups.csv')\n",
    "test_groups = pd.read_csv('/mnt/data/test_groups.csv')\n",
    "sample_submission = pd.read_csv('/mnt/data/sample_submission.csv')\n",
    "docs_titles = pd.read_csv('/mnt/data/docs_titles.tsv', sep='\\t')\n",
    "\n",
    "# Объединение заголовков с данными групп\n",
    "train_data = train_groups.merge(docs_titles, on='doc_id')\n",
    "test_data = test_groups.merge(docs_titles, on='doc_id', how='left')\n",
    "\n",
    "# Проверка наличия пропусков и их заполнение\n",
    "train_data['title'].fillna('', inplace=True)\n",
    "test_data['title'].fillna('', inplace=True)\n",
    "\n",
    "# Предобработка данных\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('russian')) | set(stopwords.words('english'))\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'<.*?>', '', text)  # Удаление HTML-тегов\n",
    "    text = re.sub(r'[^a-zA-Zа-яА-Я0-9\\s]', '', text.lower())  # Удаление спецсимволов\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stop_words and not token.isdigit()]\n",
    "    tokens = [morph.parse(word)[0].normal_form for word in tokens]\n",
    "    return ' '.join(tokens)  # Изменено на возврат строки\n",
    "\n",
    "train_data['title_processed'] = train_data['title'].apply(preprocess_text)\n",
    "test_data['title_processed'] = test_data['title'].apply(preprocess_text)\n",
    "\n",
    "# Векторизация с помощью BERT\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "\n",
    "train_data['title_embeddings'] = train_data['title_processed'].apply(lambda x: get_bert_embeddings(x))\n",
    "test_data['title_embeddings'] = test_data['title_processed'].apply(lambda x: get_bert_embeddings(x))\n",
    "\n",
    "# Создание фичей\n",
    "def embeddings_to_features(data, column_prefix):\n",
    "    embeddings = np.stack(data[column_prefix + '_embeddings'].values)\n",
    "    feature_names = [f\"{column_prefix}_embedding_{i}\" for i in range(embeddings.shape[1])]\n",
    "    features_df = pd.DataFrame(embeddings, columns=feature_names, index=data.index)\n",
    "    return features_df\n",
    "\n",
    "train_features = embeddings_to_features(train_data, 'title')\n",
    "test_features = embeddings_to_features(test_data, 'title')\n",
    "\n",
    "def add_new_features(data):\n",
    "    data['title_length'] = data['title'].apply(lambda x: len(x.split()))\n",
    "    data['unique_words'] = data['title_processed'].apply(lambda x: len(set(x.split())))\n",
    "    return data\n",
    "\n",
    "train_data = add_new_features(train_data)\n",
    "test_data = add_new_features(test_data)\n",
    "\n",
    "# Косинусные сходства Tfidf\n",
    "def vectorize_group(group):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=preprocess_text)\n",
    "    vectors = vectorizer.fit_transform(group['title'])\n",
    "    return vectors\n",
    "\n",
    "def cosine_matrix_group(group):\n",
    "    tfidf_matrix = vectorize_group(group)\n",
    "    cosine_matrix = cosine_similarity(tfidf_matrix)\n",
    "    return cosine_matrix\n",
    "\n",
    "def calc_cosine_similarity(data_grouped, count=10):\n",
    "    similarity_features_list = []\n",
    "    for name, group in tqdm(data_grouped, desc=\"Processing groups\"):\n",
    "        cosine_matrix = cosine_matrix_group(group)\n",
    "        for k, (idx, row) in enumerate(group.iterrows()):\n",
    "            similarities = []\n",
    "            for j in range(len(group)):\n",
    "                if k == j:\n",
    "                    continue\n",
    "                similarities.append(cosine_matrix[k, j])\n",
    "            top_similarities = sorted(similarities, reverse=True)[:count]\n",
    "            similarity_record = [row['pair_id']] + top_similarities\n",
    "            similarity_features_list.append(similarity_record)\n",
    "    similarity_columns = ['pair_id'] + [f'top_{i + 1}_similarity' for i in range(count)]\n",
    "    similarity_features = pd.DataFrame(similarity_features_list, columns=similarity_columns)\n",
    "    return similarity_features\n",
    "\n",
    "# Кластеризация\n",
    "def clustering_features(data_grouped, eps=0.5, min_samples=5, metric='cosine'):\n",
    "    clustering_features_list = []\n",
    "    for name, group in tqdm(data_grouped, desc=\"Processing groups\"):\n",
    "        cosine_matrix = cosine_matrix_group(group)\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric=metric).fit(cosine_matrix)\n",
    "        cluster_labels = dbscan.labels_\n",
    "        for k, (idx, row) in enumerate(group.iterrows()):\n",
    "            clustering_record = [row['pair_id'], cluster_labels[k]]\n",
    "            clustering_features_list.append(clustering_record)\n",
    "    clustering_columns = ['pair_id', 'cluster']\n",
    "    clustering_features = pd.DataFrame(clustering_features_list, columns=clustering_columns)\n",
    "    return clustering_features\n",
    "\n",
    "# Объединение данных и фичей в датасет\n",
    "train_data_grouped = train_data.groupby('group_id')\n",
    "test_data_grouped = test_data.groupby('group_id')\n",
    "\n",
    "train_similarity_features = calc_cosine_similarity(train_data_grouped)\n",
    "train_clustering_features = clustering_features(train_data_grouped)\n",
    "\n",
    "test_similarity_features = calc_cosine_similarity(test_data_grouped)\n",
    "test_clustering_features = clustering_features(test_data_grouped)\n",
    "\n",
    "train_similarity_features = train_similarity_features.fillna(0)\n",
    "test_similarity_features = test_similarity_features.fillna(0)\n",
    "\n",
    "train_data = train_data.merge(train_similarity_features, on=['pair_id'])\n",
    "train_data = train_data.merge(train_clustering_features, on=['pair_id'])\n",
    "\n",
    "test_data = test_data.merge(test_similarity_features, on=['pair_id'])\n",
    "test_data = test_data.merge(test_clustering_features, on=['pair_id'])\n",
    "\n",
    "train_data = pd.concat([train_data, train_features], axis=1)\n",
    "test_data = pd.concat([test_data, test_features], axis=1)\n",
    "\n",
    "drop_columns = ['title_embeddings']\n",
    "train_data.drop(columns=[col for col in drop_columns if col in train_data.columns], inplace=True)\n",
    "test_data.drop(columns=[col for col in drop_columns if col in test_data.columns], inplace=True)\n",
    "\n",
    "X_train = train_data.drop(columns=['doc_id', 'pair_id', 'group_id', 'target', 'title', 'title_processed'], axis=1)\n",
    "y_train = train_data['target']\n",
    "\n",
    "X_test = test_data.drop(columns=['doc_id', 'pair_id', 'group_id', 'title', 'title_processed'], axis=1)\n",
    "\n",
    "# Скейлинг данных\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Подбор гиперпараметров для CatBoost с использованием Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 1e-1),\n",
    "        'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-1, 10),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 255),\n",
    "        'random_strength': trial.suggest_loguniform('random_strength', 1e-3, 10),\n",
    "        'bagging_temperature': trial.suggest_loguniform('bagging_temperature', 1e-3, 10),\n",
    "        'od_type': trial.suggest_categorical('od_type', ['IncToDec', 'Iter']),\n",
    "        'od_wait': trial.suggest_int('od_wait', 10, 50),\n",
    "    }\n",
    "\n",
    "    train_pool = Pool(X_train_scaled, y_train)\n",
    "    cv_results = catboost.cv(\n",
    "        train_pool,\n",
    "        params,\n",
    "        fold_count=5,\n",
    "        early_stopping_rounds=50,\n",
    "        stratified=True,\n",
    "        verbose=False,\n",
    "        plot=False\n",
    "    )\n",
    "\n",
    "    return cv_results['test-F1-mean'].max()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50, n_jobs=-1)\n",
    "\n",
    "best_params = study.best_params\n",
    "\n",
    "# Обучение CatBoost с лучшими параметрами на полном наборе данных\n",
    "final_cat_model = CatBoostClassifier(**best_params)\n",
    "final_cat_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Настройка параметров мета-модели\n",
    "meta_params = {'C': [0.1, 1, 10]}\n",
    "meta_grid = GridSearchCV(LogisticRegression(), meta_params, cv=5, scoring='f1', n_jobs=-1)\n",
    "meta_grid.fit(X_train_scaled, y_train)\n",
    "best_meta = meta_grid.best_estimator_\n",
    "\n",
    "# Настройка параметров базовых моделей\n",
    "knn_params = {'n_neighbors': [3, 5, 7, 10]}\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5, scoring='f1', n_jobs=-1)\n",
    "knn_grid.fit(X_train_scaled, y_train)\n",
    "best_knn = knn_grid.best_estimator_\n",
    "\n",
    "rf_params = {'n_estimators': [100, 200, 300], 'max_depth': [None, 10, 20, 30]}\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(), rf_params, cv=5, scoring='f1', n_jobs=-1)\n",
    "rf_grid.fit(X_train_scaled, y_train)\n",
    "best_rf = rf_grid.best_estimator_\n",
    "\n",
    "svc_params = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
    "svc_grid = GridSearchCV(SVC(probability=True), svc_params, cv=5, scoring='f1', n_jobs=-1)\n",
    "svc_grid.fit(X_train_scaled, y_train)\n",
    "best_svc = svc_grid.best_estimator_\n",
    "\n",
    "# Стекинг\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('knn', best_knn),\n",
    "        ('rf', best_rf),\n",
    "        ('svc', best_svc),\n",
    "        ('cat', final_cat_model)\n",
    "    ],\n",
    "    final_estimator=best_meta,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Предсказание и сохранение результатов\n",
    "test_predictions = stacking_model.predict(X_test_scaled)\n",
    "\n",
    "# Сохранение результатов\n",
    "submission = test_groups[['pair_id']].copy()\n",
    "submission['target'] = test_predictions\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Файл с предсказаниями создан: submission.csv')\n",
    "\n",
    "# Важность признаков\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances = final_cat_model.get_feature_importance(type='PredictionValuesChange')\n",
    "feature_importances = pd.Series(importances, index=X_train.columns).sort_values()[-15:]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importances.index, feature_importances.values)\n",
    "plt.title('CatBoost Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
