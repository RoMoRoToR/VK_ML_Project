{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:51:45.470351Z",
     "start_time": "2024-04-04T21:51:45.456273Z"
    }
   },
   "source": [
    "mport pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mlp\n",
    "from sklearn.svm import SVC"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:51:45.723345Z",
     "start_time": "2024-04-04T21:51:45.692269Z"
    }
   },
   "id": "e5b55c0c9ff3f970",
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    # Убедитесь, что текст не является NaN или другим нестроковым типом данных\n",
    "    if pd.isnull(text):\n",
    "        return set()\n",
    "    # Токенизация\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Получение списка стоп-слов\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Удаление стоп-слов\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Лемматизация\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    return set(lemmatized_tokens)\n",
    "# Функция для подсчета количества ключевых действий, присутствующих в предсказанном плане\n",
    "def count_key_actions(optimal_plan, predicted_plan):\n",
    "    optimal_actions = preprocess_text(optimal_plan)\n",
    "    predicted_actions = preprocess_text(predicted_plan)\n",
    "    # Подсчет количества ключевых действий, присутствующих в обоих планах\n",
    "    common_actions = optimal_actions.intersection(predicted_actions)\n",
    "    return len(common_actions), len(optimal_actions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:51:45.774063Z",
     "start_time": "2024-04-04T21:51:45.751249Z"
    }
   },
   "id": "2f84b9f2f66d1a0d",
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_data_path = 'train_data_en.csv'\n",
    "test_data_path = 'no_labels.csv'\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "# Применяем функцию подсчета ключевых действий к каждой строке данных в тренировочном датасете\n",
    "train_data['optimal_plan_processed'] = train_data['Оптимальный план en'].apply(preprocess_text)\n",
    "train_data['predicted_plan_processed'] = train_data['Предсказанный план'].apply(preprocess_text)\n",
    "\n",
    "test_data['optimal_plan_processed'] = test_data['Оптимальный план en'].apply(preprocess_text)\n",
    "test_data['predicted_plan_processed'] = test_data['Предсказанный план'].apply(preprocess_text)\n",
    "# Отображаем обновленный тренировочный датасет\n",
    "train_data.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:52:33.657917Z",
     "start_time": "2024-04-04T21:51:45.873852Z"
    }
   },
   "id": "84664d2efbc44402",
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:52:33.699352Z",
     "start_time": "2024-04-04T21:52:33.657917Z"
    }
   },
   "id": "e28b296c47466b78",
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "# Загрузите предварительно обученный токенизатор и модель BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased', num_labels=2) \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:52:35.843634Z",
     "start_time": "2024-04-04T21:52:33.699352Z"
    }
   },
   "id": "22970433c04977aa",
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    # Токенизируем текст, добавляем специальные токены для BERT и преобразуем в тензоры PyTorch\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # Получаем эмбеддинги с помощью модели BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Возвращает эмбеддинги последнего слоя для первого токена (CLS токена)\n",
    "    # Это может быть использовано как агрегированное представление всего входного текста\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T21:52:35.857126Z",
     "start_time": "2024-04-04T21:52:35.843634Z"
    }
   },
   "id": "3cb8962bd0aece7a",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T21:52:35.871986Z",
     "start_time": "2024-04-04T21:52:35.857126Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.is_available()",
   "id": "7d8e75d42ed85a71",
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Предполагается, что train_data - это ваш DataFrame\n",
    "# Применение функции к столбцу 'text_column_name' и сохранение результатов в новый столбец 'bert_embeddings'\n",
    "train_data['predicted_plan_embeddings'] = train_data['Предсказанный план'].apply(lambda x: get_bert_embeddings(x).numpy())\n",
    "train_data['optimal_plan_embeddings'] = train_data['Оптимальный план en'].apply(lambda x: get_bert_embeddings(x).numpy())\n",
    "# После выполнения этого шага каждая строка в 'bert_embeddings' будет содержать вектор эмбеддингов для соответствующего текста\n",
    "train_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T22:25:55.161409Z",
     "start_time": "2024-04-04T21:52:35.873006Z"
    }
   },
   "id": "c934cdc4146057c3",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T22:46:39.035077Z",
     "start_time": "2024-04-04T22:25:55.161409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_data['predicted_plan_embeddings'] = test_data['Предсказанный план'].apply(lambda x: get_bert_embeddings(x).numpy())\n",
    "test_data['optimal_plan_embeddings'] = test_data['Оптимальный план en'].apply(lambda x: get_bert_embeddings(x).numpy())\n",
    "# После выполнения этого шага каждая строка в 'bert_embeddings' будет содержать вектор эмбеддингов для соответствующего текста\n",
    "test_data.head()"
   ],
   "id": "3d81c42d160d6de2",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T22:46:39.510851Z",
     "start_time": "2024-04-04T22:46:39.035077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Предполагается, что train_data уже содержит столбцы с эмбеддингами\n",
    "# Преобразуем эмбеддинги из списка в отдельные признаки\n",
    "def embeddings_to_features(data, column_prefix):\n",
    "    embeddings = np.stack(data[column_prefix + '_embeddings'].values)\n",
    "    feature_names = [f\"{column_prefix}_embedding_{i}\" for i in range(embeddings.shape[1])]\n",
    "    features_df = pd.DataFrame(embeddings, columns=feature_names, index=data.index)\n",
    "    return features_df\n",
    "\n",
    "# Преобразование эмбеддингов в признаки\n",
    "optimal_plan_features = embeddings_to_features(train_data, 'optimal_plan')\n",
    "predicted_plan_features = embeddings_to_features(train_data, 'predicted_plan')\n",
    "\n",
    "# Объединяем все признаки в один DataFrame\n",
    "features = pd.concat([optimal_plan_features, predicted_plan_features], axis=1)\n",
    "\n",
    "# Предположим, что целевая переменная называется 'success'\n",
    "labels = train_data['Успех предсказанного плана']\n",
    "\n",
    "# Разбиение данных на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n"
   ],
   "id": "756c475904ec6193",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T22:46:46.219145Z",
     "start_time": "2024-04-04T22:46:39.512366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Создаем и обучаем модель\n",
    "model = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.05, num_leaves=31, objective='binary')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Делаем предсказания на тестовом наборе\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Вычисляем точность\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Точность модели: {accuracy:.2f}\")\n"
   ],
   "id": "86ddbf244d3b3287",
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T23:03:47.817258Z",
     "start_time": "2024-04-04T23:03:47.783268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "t_data = train_data\n",
    "ts_data = test_data\n",
    "# Для train_data\n",
    "X_train_data = [np.concatenate([optimal, predicted]) for optimal, predicted in zip(t_data['optimal_plan_embeddings'].values, t_data['predicted_plan_embeddings'].values)]\n",
    "\n",
    "y_train_data = t_data['Успех предсказанного плана'].values\n",
    "\n",
    "# Для test_data\n",
    "X_test_data = [np.concatenate([optimal, predicted]) for optimal, predicted in zip(ts_data['optimal_plan_embeddings'].values, ts_data['predicted_plan_embeddings'].values)]\n",
    "# y_test_data = t_data['Успех предсказанного плана'].values\n",
    "\n",
    "# Теперь X_train_data и X_test_data содержат конкатенированные векторы признаков\n",
    "# Используйте X_train_data и y_train_data для разделения на обучающую и валидационную выборки\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_data, y_train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# После этого, вы можете приступить к обучению модели, используя X_train и y_train\n",
    "# А X_test_data и y_test_data использовать для тестирования модели\n"
   ],
   "id": "4c4be516f7deef5",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T23:03:55.726912Z",
     "start_time": "2024-04-04T23:03:54.714775Z"
    }
   },
   "cell_type": "code",
   "source": [
    " class VectorDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "  \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx]\n",
    "\n",
    "train_dataset = VectorDataset(X_train, y_train)\n",
    "test_dataset = TestDataset(X_test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "print(type(test_dataset))\n",
    "print(type(train_dataset))"
   ],
   "id": "8ebe760916dd68d2",
   "execution_count": 33,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T23:04:09.011721Z",
     "start_time": "2024-04-04T23:04:08.966830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 1024)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Определим размер входа как размерность одного вектора эмбеддинга\n",
    "input_size = X_train.shape[1]\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "model = SimpleNN(input_size, num_classes)\n",
    "print(model)\n",
    "print(\"Input size:\", input_size, \"Number of classes:\", num_classes)"
   ],
   "id": "71c7dad37f815e7e",
   "execution_count": 34,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T23:05:34.215454Z",
     "start_time": "2024-04-04T23:04:12.668693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "# Обучение модели\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "\n"
   ],
   "id": "4b9b976858c9452f",
   "execution_count": 35,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T23:07:24.794468Z",
     "start_time": "2024-04-04T23:07:24.559401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Переключение модели в режим оценки для тестирования\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():  # Отключение вычисления градиентов для ускорения\n",
    "    for features in test_loader:\n",
    "        features = features.to(device)\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Вывод первых 10 предсказаний\n",
    "print(\"Test predictions for the first 10 samples:\", test_predictions[:])\n",
    "# Создание DataFrame из предсказаний\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Test Predictions': test_predictions\n",
    "})\n",
    "\n",
    "# Сохранение DataFrame в CSV\n",
    "predictions_df.to_csv('test_predictions.csv', index=False)"
   ],
   "id": "87881454f562df67",
   "execution_count": 37,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
